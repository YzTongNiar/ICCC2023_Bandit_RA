{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%reset -f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import kl_ucb_policy\n",
    "import ts_bandit_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi: [0.99 0.98 0.96 0.93 0.9  0.1  0.06 0.04]\n",
      "Delta: [15.66 12.78 10.08  4.86  0.   18.   18.72 19.44]\n"
     ]
    }
   ],
   "source": [
    "#Parameters for TS test\n",
    "p = np.array([.99, .98, .96, .93, 0.90, .10, .06, .04]) #Bernoulli Parameters\n",
    "# p = np.array([.95, .90, .80, .65, .45, .25, .15, .10])\n",
    "# p = np.array([.90, .80, .70, .55, .45, .35, .20, .10])\n",
    "rate = np.array([6, 9, 12, 18, 24, 36, 48, 54])\n",
    "tp = np.multiply(p,rate)\n",
    "\n",
    "K= p.shape[0] #Number of branches\n",
    "Delta= ( np.ones(K)*np.max(tp) ) - tp\n",
    "s = ts_bandit_policy.construct_s(rate, K)\n",
    "\n",
    "\n",
    "T= 5000 #Time periods\n",
    "runs = 10 #Number of iterations\n",
    "\n",
    "print(\"Pi:\",p)\n",
    "print(\"Delta:\",Delta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Thompson Sampling Bandit Simulation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Academic\\RateControl\\Bandit_UCB\\kl_ucb_policy.py:153: RuntimeWarning: invalid value encountered in true_divide\n",
      "  r_emp = (self.S/self.N)*self.rate*self.Sig_S\n",
      "D:\\Academic\\RateControl\\Bandit_UCB\\kl_ucb_policy.py:58: RuntimeWarning: divide by zero encountered in log\n",
      "  upperbound = np.log(t)/N[k]\n",
      "D:\\Academic\\RateControl\\Bandit_UCB\\kl_ucb_policy.py:59: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  reward = S[k]/N[k]\n",
      "D:\\Academic\\RateControl\\Bandit_UCB\\kl_ucb_policy.py:186: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  self.Phi[k,k] = self.S[k]/self.N[k]*self.rate[k]\n",
      "D:\\Academic\\RateControl\\Bandit_UCB\\kl_ucb_policy.py:58: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  upperbound = np.log(t)/N[k]\n",
      "D:\\Academic\\RateControl\\Bandit_UCB\\kl_ucb_policy.py:58: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  upperbound = np.log(t)/N[k]\n",
      "D:\\Academic\\RateControl\\Bandit_UCB\\kl_ucb_policy.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  result = p * np.log(p/q) + (1-p)*np.log((1-p)/(1-q))\n",
      "D:\\Academic\\RateControl\\Bandit_UCB\\kl_ucb_policy.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  result = p * np.log(p/q) + (1-p)*np.log((1-p)/(1-q))\n",
      "D:\\Academic\\RateControl\\Bandit_UCB\\kl_ucb_policy.py:8: RuntimeWarning: invalid value encountered in log\n",
      "  result = p * np.log(p/q) + (1-p)*np.log((1-p)/(1-q))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 20>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     41\u001B[0m rewards_c_ts[arm_c_ts, t] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mbinomial(\u001B[38;5;241m1\u001B[39m, p[arm_c_ts]) \u001B[38;5;241m*\u001B[39m rate[arm_c_ts]\n\u001B[0;32m     42\u001B[0m c_ts\u001B[38;5;241m.\u001B[39mupdate_state(arm_c_ts, \u001B[38;5;28mint\u001B[39m(rewards_c_ts[arm_c_ts, t] \u001B[38;5;241m/\u001B[39m rate[arm_c_ts]))\n\u001B[1;32m---> 44\u001B[0m arm_ts \u001B[38;5;241m=\u001B[39m \u001B[43mts\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_next_arm\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m actions_ts[arm_ts, t] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     46\u001B[0m rewards_ts[arm_ts, t] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mbinomial(\u001B[38;5;241m1\u001B[39m, p[arm_ts]) \u001B[38;5;241m*\u001B[39m rate[arm_ts]\n",
      "File \u001B[1;32mD:\\Academic\\RateControl\\Bandit_UCB\\ts_bandit_policy.py:26\u001B[0m, in \u001B[0;36mTSBanditPolicy.select_next_arm\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     24\u001B[0m p_avail \u001B[38;5;241m=\u001B[39m p\u001B[38;5;241m.\u001B[39mcopy()\u001B[38;5;241m*\u001B[39m(p\u001B[38;5;241m>\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelta) \u001B[38;5;66;03m# available rates with PER larger than delta\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28msum\u001B[39m(p_avail) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m---> 26\u001B[0m     selected_arm \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmultiply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp_avail\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrate\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     28\u001B[0m     selected_arm \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(p)\n",
      "File \u001B[1;32m<__array_function__ internals>:5\u001B[0m, in \u001B[0;36margmax\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32mD:\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1195\u001B[0m, in \u001B[0;36margmax\u001B[1;34m(a, axis, out)\u001B[0m\n\u001B[0;32m   1121\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_argmax_dispatcher)\n\u001B[0;32m   1122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21margmax\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1123\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1124\u001B[0m \u001B[38;5;124;03m    Returns the indices of the maximum values along an axis.\u001B[39;00m\n\u001B[0;32m   1125\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1193\u001B[0m \n\u001B[0;32m   1194\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1195\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43margmax\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001B[0m, in \u001B[0;36m_wrapfunc\u001B[1;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m bound(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001B[39;00m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001B[39;00m\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;66;03m# exception has a traceback chain.\u001B[39;00m\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "c_ts = ts_bandit_policy.CBanditPolicy(K, rate, s, 0)\n",
    "total_rewards_list_c_ts = np.zeros((runs, T))\n",
    "actions_list_c_ts = []\n",
    "\n",
    "ts = ts_bandit_policy.TSBanditPolicy(K, rate, 0)\n",
    "total_rewards_list_ts = np.zeros((runs, T))\n",
    "actions_list_ts = []\n",
    "\n",
    "cklucb = kl_ucb_policy.C_KLUCB(K, rate, s) #Original KL UCB\n",
    "total_rewards_list_cklucb = np.zeros((runs, T))\n",
    "actions_list_cklucb = []\n",
    "\n",
    "klucb = kl_ucb_policy.KLUCBPolicy(K, rate) #Original KL UCB\n",
    "total_rewards_list_klucb = np.zeros((runs, T))\n",
    "actions_list_klucb = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for run in range(runs):\n",
    "    c_ts.reset()\n",
    "    actions_c_ts = np.zeros((K, T), dtype=int)\n",
    "    rewards_c_ts = np.zeros((K, T), dtype=float)\n",
    "\n",
    "    ts.reset()\n",
    "    actions_ts = np.zeros((K, T), dtype=int)\n",
    "    rewards_ts = np.zeros((K, T), dtype=float)\n",
    "\n",
    "    cklucb.reset()\n",
    "    actions_cklucb = np.zeros((K, T), dtype=int)\n",
    "    rewards_cklucb = np.zeros((K, T), dtype=float)\n",
    "\n",
    "    klucb.reset()\n",
    "    actions_klucb = np.zeros((K, T), dtype=int)\n",
    "    rewards_klucb = np.zeros((K, T), dtype=float)\n",
    "\n",
    "    for t in range(T):\n",
    "        c_ts.reduce_set()\n",
    "        arm_c_ts = c_ts.select_next_arm()\n",
    "        actions_c_ts[arm_c_ts, t] = 1\n",
    "        rewards_c_ts[arm_c_ts, t] = np.random.binomial(1, p[arm_c_ts]) * rate[arm_c_ts]\n",
    "        c_ts.update_state(arm_c_ts, int(rewards_c_ts[arm_c_ts, t] / rate[arm_c_ts]))\n",
    "\n",
    "        arm_ts = ts.select_next_arm()\n",
    "        actions_ts[arm_ts, t] = 1\n",
    "        rewards_ts[arm_ts, t] = np.random.binomial(1, p[arm_ts]) * rate[arm_ts]\n",
    "        ts.update_state(arm_ts, int(rewards_ts[arm_ts, t] / rate[arm_ts]))\n",
    "\n",
    "        cklucb.reduce_set()\n",
    "        arm_cklucb = cklucb.select_next_arm()\n",
    "        actions_cklucb[arm_cklucb, t] = 1\n",
    "        rewards_cklucb[arm_cklucb, t] = np.random.binomial(1, p[arm_cklucb]) * rate[arm_cklucb]\n",
    "        cklucb.update_state(arm_cklucb, int(rewards_cklucb[arm_cklucb, t] / rate[arm_cklucb]))\n",
    "\n",
    "        arm_klucb = klucb.select_next_arm()\n",
    "        actions_klucb[arm_klucb, t] = 1\n",
    "        rewards_klucb[arm_klucb, t] = np.random.binomial(1, p[arm_klucb]) * rate[arm_klucb]\n",
    "        klucb.update_state(arm_klucb, rewards_klucb[arm_klucb, t] / rate[arm_klucb])\n",
    "\n",
    "    cumulative_rewards_c_ts = np.cumsum(rewards_c_ts, axis=1) #Cumulative rewards of each arm according to time\n",
    "    total_rewards_c_ts = np.sum(cumulative_rewards_c_ts, axis=0) #Cumulative rewards of all arms according to time\n",
    "    total_rewards_list_c_ts[run, :] = np.copy(total_rewards_c_ts)\n",
    "    actions_list_c_ts.append(np.copy(actions_c_ts))\n",
    "\n",
    "    cumulative_rewards_ts = np.cumsum(rewards_ts, axis=1) #Cumulative rewards of each arm according to time\n",
    "    total_rewards_ts = np.sum(cumulative_rewards_ts, axis=0) #Cumulative rewards of all arms according to time\n",
    "    total_rewards_list_ts[run, :] = np.copy(total_rewards_ts)\n",
    "    actions_list_ts.append(np.copy(actions_ts))\n",
    "\n",
    "    cumulative_rewards_cklucb = np.cumsum(rewards_cklucb, axis=1) #Cumulative rewards of each arm according to time\n",
    "    total_rewards_cklucb = np.sum(cumulative_rewards_cklucb, axis=0) #Cumulative rewards of all arms according to time\n",
    "    total_rewards_list_cklucb[run, :] = np.copy(total_rewards_cklucb)\n",
    "    actions_list_cklucb.append(np.copy(actions_cklucb))\n",
    "\n",
    "    cumulative_rewards_klucb = np.cumsum(rewards_klucb, axis=1) #Cumulative rewards of each arm according to time\n",
    "    total_rewards_klucb = np.sum(cumulative_rewards_klucb, axis=0) #Cumulative rewards of all arms according to time\n",
    "    total_rewards_list_klucb[run, :] = np.copy(total_rewards_klucb)\n",
    "    actions_list_klucb.append(np.copy(actions_klucb))\n",
    "\n",
    "time_spent = time.time() - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulation Result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(\"Time for Thompson Sampling, with T =\", T, \", runs =\", runs, \":\", (time_spent), \"seconds\")\n",
    "#\n",
    "# ## Average total rewards\n",
    "# mean_total_rewards_c_ts = np.mean(total_rewards_list_c_ts, axis=0)\n",
    "# mean_total_rewards_ts = np.mean(total_rewards_list_ts, axis=0)\n",
    "# mean_total_rewards_klucb = np.mean(total_rewards_list_klucb, axis=0)\n",
    "# print(\"t & Reward (total) c_ts :\", t, mean_total_rewards_c_ts[t])\n",
    "# print(\"t & Reward (total) ts :\", t, mean_total_rewards_ts[t])\n",
    "# print(\"t & Reward (total) KL-UCB :\", t, mean_total_rewards_klucb[t])\n",
    "#\n",
    "# fig = plt.figure(figsize=(12, 8))\n",
    "# ax1 = fig.add_subplot(1, 1, 1)\n",
    "# ax1.plot(mean_total_rewards_c_ts, linestyle='-', label='Average total rewards of C-TS')\n",
    "# ax1.plot(mean_total_rewards_ts, linestyle='-', label='Average total rewards of TS')\n",
    "# ax1.plot(mean_total_rewards_klucb, linestyle='-', label='Average total rewards of KL-UCB')\n",
    "# ax1.legend(loc='best')\n",
    "# ax1.set_title('Evolution of cumulative rewards according to time')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Regrets calculation for TS\n",
    "total_action_c_ts = np.zeros((K, T))\n",
    "for actions_c_ts in actions_list_c_ts:\n",
    "    total_action_c_ts += np.cumsum(actions_c_ts, axis=1)  #The cumulative times of each arm to be selected\n",
    "total_action_c_ts = total_action_c_ts / runs\n",
    "regret_cumule_c_ts = np.dot(Delta, total_action_c_ts[:, :])  # Cumulative regrets\n",
    "\n",
    "total_action_ts = np.zeros((K,T))\n",
    "for actions_ts in actions_list_ts:\n",
    "    total_action_ts += np.cumsum(actions_ts, axis=1)  #The cumulative times of each arm to be selected\n",
    "total_action_ts = total_action_ts / runs\n",
    "regret_cumule_ts = np.dot(Delta, total_action_ts[:, :])  # Cumulative regrets\n",
    "\n",
    "total_action_cklucb = np.zeros((K, T))\n",
    "for actions_cklucb in actions_list_cklucb:\n",
    "    total_action_cklucb += np.cumsum(actions_cklucb, axis=1) #The cumulative times of each arm to be selected\n",
    "total_action_cklucb = total_action_cklucb / runs\n",
    "regret_cumule_cklucb = np.dot(Delta, total_action_cklucb[:, :]) # Cumulative regrec_ts\n",
    "\n",
    "total_action_klucb = np.zeros((K, T))\n",
    "for actions_klucb in actions_list_klucb:\n",
    "    total_action_klucb += np.cumsum(actions_klucb, axis=1) #The cumulative times of each arm to be selected\n",
    "total_action_klucb = total_action_klucb / runs\n",
    "regret_cumule_klucb = np.dot(Delta, total_action_klucb[:, :]) # Cumulative regrec_ts\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "ax1.plot(regret_cumule_c_ts[:], linestyle='-', label='C-TS')\n",
    "ax1.plot(regret_cumule_ts[:], linestyle='-', label='TS')\n",
    "ax1.plot(regret_cumule_cklucb[:],linestyle='--', label='C-KLUCB')\n",
    "ax1.plot(regret_cumule_klucb[:],linestyle='--', label='KLUCB')\n",
    "\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid()\n",
    "ax1.set_title('Evolution of average cumulative regrets according to time')\n",
    "#ax1.xscale('log')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}